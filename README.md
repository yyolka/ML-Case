# Прогнозирование стоимости недвижимости (Sberbank Russian Housing Market)

Этот проект представляет собой решение задачи регрессии с соревнования Kaggle [Sberbank Russian Housing Market](https://www.kaggle.com/c/sberbank-russian-housing-market).  
Цель — предсказать рыночную стоимость квартир в России на основе набора признаков, включающего характеристики объектов недвижимости и макроэкономические показатели.

## Данные

- **train (1).csv** – около 30 000 транзакций с ~300 признаками (характеристики квартир, статистика по районам, расстояния до инфраструктуры).
- **macro.csv** – около 100 макроэкономических индикаторов (цены на нефть, курсы валют, ВВП, ставки по ипотеке и т.д.), привязанных к дате сделки.

После объединения данных был проведён тщательный анализ и проектирование признаков.

## Основные этапы

### 1. Создание новых признаков (Feature Engineering)
- **Возраст дома** – `year` (из `timestamp`) минус `build_year`.
- **Отношения площадей** – `life_sq / full_sq`, `kitch_sq / full_sq`.
- **Индикаторы этажа** – первый, последний, средний этаж.
- **Близость к инфраструктуре** – метро в пределах 1 км, парк в пределах 2 км.
- **Целевое кодирование (Target Encoding)** – для района (`sub_area`) с большим числом уникальных значений (сглаженное среднее цены с параметром `alpha=10`).

### 2. Очистка и отбор признаков
- Удалены признаки с нулевой или очень низкой дисперсией (`VarianceThreshold(threshold=0.01)`).
- Удалены высококоррелированные признаки (коэффициент Пирсона > 0.75) для борьбы с мультиколлинеарностью.
- Вручную отброшены заведомо неинформативные колонки (`ID_*`, `school_quota`, `material` и др.).
- Ограничены выбросы в целевой переменной – оставлены только цены между 5-м и 95-м процентилями.

### 3. Кодирование категориальных признаков
- Бинарные признаки типа `yes`/`no` преобразованы в 0/1.
- Порядковая переменная `ecology` закодирована числами от 1 до 4.
- Для `sub_area` выполнено целевое кодирование (smoothed mean encoding) вместо one-hot, чтобы избежать проклятия размерности.

### 4. Моделирование
Для каждой модели был создан пайплайн с `StandardScaler` (масштабирование числовых признаков). Рассмотрены следующие модели:
- Линейная регрессия
- Lasso-регрессия (с небольшим alpha)
- Ridge-регрессия (с небольшим alpha)
- **CatBoostRegressor** (градиентный бустинг, устойчивый к категориальным признакам, хотя после кодирования остались только числовые).

Гиперпараметры CatBoost подбирались с помощью `GridSearchCV` (3-кратная кросс‑валидация) с метрикой `neg_root_mean_squared_error`.

## Результаты

| Модель           | RMSE (руб.)  | MAPE (%) |
|------------------|-------------:|---------:|
| Linear Regression | 1 667 392    | 20.02    |
| Lasso (α=0.0001)  | 1 667 392    | 20.02    |
| Ridge (α=0.0001)  | 1 667 392    | 20.02    |
| **CatBoost**      | **1 202 667** | **12.26** |

CatBoost значительно превосходит линейные модели, снижая относительную ошибку до 12.26%.

**Лучшие параметры CatBoost:**
- `depth`: 8
- `learning_rate`: 0.05
- `iterations`: 1000

## Использованные библиотеки
- Python 3.13
- pandas, numpy – обработка данных
- scikit-learn – пайплайны, предобработка, отбор признаков, метрики
- CatBoost – градиентный бустинг
- matplotlib, seaborn – визуализация
- catboost

## Как запустить проект
1. Склонируйте репозиторий.
2. Установите зависимости:
   ```bash
   pip install pandas numpy scikit-learn catboost matplotlib seaborn catboost
